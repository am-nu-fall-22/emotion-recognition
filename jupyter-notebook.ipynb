{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition\n",
    "\n",
    "### Abstract\n",
    "\n",
    "In this project, based on an input image, the basic facial expressions such as happiness, surprise, sadness, and anger will be detected. To achieve this, first, important facial landmarks will be extracted and then using the location of these landmarks, some features will be constructed for training a dataset using Support Vector Machine (SVM) algorithm. The resulting classifier can be used for predicting facial expressions in an image.\n",
    "\n",
    "### Dataset\n",
    "To use a supervised ML algorithm, a dataset with face images and appropriate labels is needed. For this, the famous Cohn-Kanade dataset is employed in this project. Since it's a private dataset, a free version of it found on [github](https://github.com/spenceryee/CS229/tree/master/CK%2B) is used. This dataset consists of 8 directories each containing face images of a specific emotion. The emotions are: anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. Each set has different number of images with neutral having the largest one. Having roughly the same number of images in each set might help us achieve better results.\n",
    "\n",
    "### Facial Landmarks\n",
    "We need some features from each face image to train a classifier. For this, the standard dlib library is used to detect faces in an image. After detecting faces, 68 facial landmarks is determined for each face. Further info on how to extract the landmarks can be found [here](https://ieeexplore.ieee.org/document/6248015). In this project, dlib's shape predictor is used to detect facial landmarks. The file to predict landmarks can be found [here](https://github.com/italojs/facial-landmarks-recognition/blob/master/shape_predictor_68_face_landmarks.dat).\n",
    "\n",
    "### Feature Extraction\n",
    "Since facial expression of a person largely depends on the relative locations of the facial landmarks with respect each other, they are considered good candidates to be included in the feature vector. To better determine the relative position of landmark points with respect to each other, they are represented with respect to a new coordinate referrence which is the mean of all the 68 facial landmark points. \n",
    "In some images, the face might not have a straight pose with respect to the vertical axis and might be in a tilted position. To solve this issue, all images in the dataset are rotated so that the faces are aligned with the vertical axis. To determine the rotation angle, 4 points from the 68 facial landmarks are considered. These points are located on the nose and can help us determine the desired rotation angle. In the picture below, the 4 points (27, 28, 29, 30) are shown.\n",
    "\n",
    "\n",
    "![](data/jupyter-data/image1.png)\n",
    "\n",
    "\n",
    "After rotating each facial landmark with the calculated rotation angle, new coordinates are calculated for each landmark with respect to the new referrence point (mean point). Next, the polar coordinates of the points are calculated. First, the distance between each point and the referrence point is calulated. Then, the angle of each point with respect to the horizontal axis is calculated. The idea to use these features is inspired by [this paper](https://arxiv.org/pdf/1603.09129.pdf).\n",
    "\n",
    "### Feature Vector\n",
    "The feature vector for each landmark point consists of 4x68=272 numbers: 68 x coordinates (cartesian coordinates wrt mean point), 68 y coordinates ((cartesian coordinates wrt mean point)), 68 distance values (r in polar coordinate wrt mean point), and 68 angle values (theta in polar coordinates wrt mean point).\n",
    "\n",
    "### Classification\n",
    "The data is split into two parts: training and prediction. 90% of the dataset images are for training and the rest is for prediction/test phase. The dataset images for each set are shuffled before selecting them for training or prediction sets.\n",
    "\n",
    "### Implementation\n",
    "In the following sections, the different python files containing implementation details are shown. The structure of the project is show in the picture below.\n",
    "\n",
    "![](data/jupyter-data/image3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### main.py\n",
    "This is the main file that should be run for training the dataset and seeing the confusion matrix and accuracy of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SVM_Data\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# initial variables\n",
    "dataset = \"dataset1\"\n",
    "classifier = \"clf_saved3.pkl\"\n",
    "confusion_matrix_file_name = \"confusion_matrix3.txt\"\n",
    "\n",
    "# linear SVM classifier\n",
    "clf = SVC(kernel='linear', probability=True, tol=1e-3)\n",
    "\n",
    "# prepare data for training and prediction sets\n",
    "training_data, training_labels, prediction_data, prediction_labels = SVM_Data.SVM_Data(\n",
    "    dataset)\n",
    "\n",
    "# change to numpy array for the classifier\n",
    "npar_train = np.array(training_data)\n",
    "npar_trainlabs = np.array(training_labels)\n",
    "npar_pred = np.array(prediction_data)\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(npar_train, training_labels)\n",
    "\n",
    "# save the classifier\n",
    "clf_saved = joblib.dump(clf, \"results/%s/%s\" % (dataset, classifier))\n",
    "\n",
    "# calculate the accuracy\n",
    "prediction_accuracy = clf.score(npar_pred, prediction_labels)\n",
    "print(\"accuracy: \", prediction_accuracy)\n",
    "\n",
    "# calculate confusion matrix\n",
    "Confusion_Matrix = np.zeros((8, 8))\n",
    "for data, label in zip(prediction_data, prediction_labels):\n",
    "    SVM_index = clf.predict(np.array(data).reshape(-1, len(data)))[0]\n",
    "    Real_index = label\n",
    "    Confusion_Matrix[Real_index][SVM_index] = Confusion_Matrix[Real_index][SVM_index] + 1\n",
    "np.savetxt('results/%s/%s' %\n",
    "           (dataset, confusion_matrix_file_name), Confusion_Matrix)\n",
    "print(Confusion_Matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM_Data.py\n",
    "This file contains details for preparing the dataset for training and prediction sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import Landmark_Detector\n",
    "import Shuffle\n",
    "\n",
    "\n",
    "def extract_feature(item):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    # read image\n",
    "    image = cv2.imread(item)\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # increase the contrast\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    # extract features\n",
    "    features_vector = Landmark_Detector.Landmark_Detector(clahe_image)\n",
    "    return features_vector\n",
    "\n",
    "\n",
    "def SVM_Data(dataset):\n",
    "    emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\",\n",
    "                \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "\n",
    "    for emotion in emotions:\n",
    "        print(\"emotion: \", emotion)\n",
    "\n",
    "        # shuffle the set\n",
    "        training, prediction = Shuffle.Shuffle(emotion, dataset)\n",
    "\n",
    "        print(\"  training ...\")\n",
    "        for item in training:\n",
    "            features_vector = extract_feature(item)\n",
    "            if features_vector == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                # append image array to training data list\n",
    "                training_data.append(features_vector[0])\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "\n",
    "        print(\"  prediction ...\")\n",
    "        for item in prediction:\n",
    "            features_vector = extract_feature(item)\n",
    "            if features_vector == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                # append image array to prediction data list\n",
    "                prediction_data.append(features_vector[0])\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "        print()\n",
    "    return training_data, training_labels, prediction_data, prediction_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle.py\n",
    "This file contains a function which shuffles the data and selects the first 90% of them for training set and the rest for prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "\n",
    "def Shuffle(emotion, dataset):\n",
    "    files = glob.glob(\"data/datasets/%s/%s/*\" % (dataset, emotion))\n",
    "    random.shuffle(files)\n",
    "    # first 90% of file list after shuffling as training data\n",
    "    training = files[:int(len(files)*0.9)]\n",
    "    # last 10% of file list after shuffling as testing data\n",
    "    prediction = files[-int(len(files)*0.1):]\n",
    "    return training, prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Landmark_Detector.py\n",
    "This files contains the details for detecting facial landmarks and preparing them to be included in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "\n",
    "def Landmark_Detector(image):\n",
    "\n",
    "    features_vector = []\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\n",
    "        \"data/shape_predictor_68_face_landmarks.dat\")\n",
    "    detections = detector(image, 1)\n",
    "\n",
    "    # repeat for all detected faces\n",
    "    for k, d in enumerate(detections):\n",
    "\n",
    "        # facial landmarks with the predictor class of the dlib library\n",
    "        shape = predictor(image, d)\n",
    "        xsequence = []\n",
    "        ysequence = []\n",
    "\n",
    "        # save x and y coordinates in two lists\n",
    "        for i in range(1, 68):\n",
    "            xsequence.append(float(shape.part(i).x))\n",
    "            ysequence.append(float(shape.part(i).y))\n",
    "\n",
    "        # mean of all landmarks' x and y coordinates\n",
    "        xmean = np.mean(xsequence)\n",
    "        ymean = np.mean(ysequence)\n",
    "\n",
    "        # distance between each point and the mean point\n",
    "        xrelative = [(x-xmean) for x in xsequence]\n",
    "        yrelative = [(y-ymean) for y in ysequence]\n",
    "\n",
    "        # prevent 'divide by 0' error\n",
    "        if xsequence[26] == xsequence[29]:\n",
    "            anglenose = 0\n",
    "        else:\n",
    "            # rotation angle\n",
    "            anglenose = int(math.atan(\n",
    "                (ysequence[26]-ysequence[29])/(xsequence[26]-xsequence[29]))*180/math.pi)\n",
    "        if anglenose < 0:\n",
    "            anglenose += 90\n",
    "        else:\n",
    "            anglenose -= 90\n",
    "\n",
    "        # create feature vector\n",
    "        feature_vector = []\n",
    "        for x, y, w, z in zip(xrelative, yrelative, xsequence, ysequence):\n",
    "            feature_vector.append(x)\n",
    "            feature_vector.append(y)\n",
    "            meannp = np.asarray((ymean, xmean))\n",
    "            coornp = np.asarray((z, w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            if (w - xmean == 0):\n",
    "                if (z - ymean > 0):\n",
    "                    anglerelative = 90 - anglenose\n",
    "                else:\n",
    "                    anglerelative = -90 - anglenose\n",
    "\n",
    "            else:\n",
    "                anglerelative = (math.atan((z-ymean)/(w-xmean))\n",
    "                                 * 180/math.pi) - anglenose\n",
    "            feature_vector.append(dist)\n",
    "            feature_vector.append(anglerelative)\n",
    "\n",
    "        # append to features_vector\n",
    "        features_vector.append(feature_vector)\n",
    "\n",
    "    # return error if no face is detected\n",
    "    if len(detections) < 1:\n",
    "        features_vector = \"error\"\n",
    "\n",
    "    return features_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test_pic.py\n",
    "This file contains details for testing a picture for emotion detection. the path of the picture must be given to predict the facial expressions on detected faces in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import joblib\n",
    "from imutils import face_utils\n",
    "import SVM_Data\n",
    "\n",
    "\n",
    "def draw_landmarks(image, detector, predictor):\n",
    "\n",
    "    # convert to grey scale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "\n",
    "        # determine the facial landmarks for the face region\n",
    "        shape = predictor(gray, rect)\n",
    "\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "def write_emotion(image, clf, detector, emotions, features_vector):\n",
    "\n",
    "    # detect faces in image\n",
    "    detections = detector(image, 1)\n",
    "\n",
    "    # enumerate on detected faces and write the emotion on top of each\n",
    "    for k, d in enumerate(detections):\n",
    "        # predict emotion\n",
    "        index = clf.predict(\n",
    "            np.array(features_vector[k]).reshape(-1, len(features_vector[k])))\n",
    "\n",
    "        # write emotion on top of each detected face\n",
    "        emotion = emotions[index[0]]\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(image, emotion, (d.left(), d.top()),\n",
    "                    font, 1, (0, 255, 0), 3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialization\n",
    "    dataset = \"dataset1\"\n",
    "    classifier = \"clf_saved.pkl\"\n",
    "    test_data_dir = \"data/test_data/\"\n",
    "    pic_name = \"test_pic3.jpg\"\n",
    "    pic_dir = test_data_dir + pic_name\n",
    "    emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\",\n",
    "                \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "    # read test image\n",
    "    image = cv2.imread(pic_dir)\n",
    "\n",
    "    # face detector\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # landmark predictor\n",
    "    predictor = dlib.shape_predictor(\n",
    "        \"data/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "    # load trained classifier\n",
    "    clf = joblib.load('results/%s/%s' % (dataset, classifier))\n",
    "\n",
    "    # extract features from image\n",
    "    features_vector = SVM_Data.extract_feature(pic_dir)\n",
    "\n",
    "    # write emotion\n",
    "    write_emotion(image, clf, detector, emotions, features_vector)\n",
    "\n",
    "    # draw landmarks\n",
    "    draw_landmarks(image, detector, predictor)\n",
    "\n",
    "    # show and save resulting image\n",
    "    cv2.namedWindow(pic_name)\n",
    "    cv2.moveWindow(pic_name, 0, 0)\n",
    "    cv2.imshow(pic_name, image)\n",
    "    cv2.imwrite('results/test_data/' + pic_name, image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Below you can see the confusion matrix and the resulting accuracy:\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "![](data/jupyter-data/image2.png)\n",
    "\n",
    "Accuracy: 0.93103448\n",
    "\n",
    "### Test on new images\n",
    "\n",
    "Image 1:\n",
    "\n",
    "![](data/jupyter-data/test_pic2.jpg)\n",
    "\n",
    "\n",
    "Image 2:\n",
    "\n",
    "![](data/jupyter-data/test_pic3.jpg)\n",
    "\n",
    "\n",
    "Image 3:\n",
    "\n",
    "![](data/jupyter-data/test_pic4.jpg)\n",
    "\n",
    "### Links to other materials\n",
    "1. Link to YouTube Video: https://youtu.be/7ht-pKO7nGI\n",
    "\n",
    "2. Link to Supplemantary Materials: https://github.com/am-nu-fall-22/emotion-recognition\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
