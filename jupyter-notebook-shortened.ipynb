{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition\n",
    "\n",
    "### Abstract\n",
    "\n",
    "In this project, based on an input image, the basic facial expressions such as happiness, surprise, sadness, and anger will be detected. To achieve this, first, important facial landmarks will be extracted and then using the location of these landmarks, some features will be constructed for training a dataset using Support Vector Machine (SVM) algorithm. The resulting classifier can be used for predicting facial expressions in an image.\n",
    "\n",
    "### Dataset\n",
    "To use a supervised ML algorithm, a dataset with face images and appropriate labels is needed. For this, the famous Cohn-Kanade dataset is employed in this project. Since it's a private dataset, a free version of it found on [github](https://github.com/spenceryee/CS229/tree/master/CK%2B) is used. This dataset consists of 8 directories each containing face images of a specific emotion. The emotions are: anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. Each set has different number of images with neutral having the largest one. Having roughly the same number of images in each set might help us achieve better results.\n",
    "\n",
    "### Facial Landmarks\n",
    "We need some features from each face image to train a classifier. For this, the standard dlib library is used to detect faces in an image. After detecting faces, 68 facial landmarks is determined for each face. Further info on how to extract the landmarks can be found [here](https://ieeexplore.ieee.org/document/6248015). In this project, dlib's shape predictor is used to detect facial landmarks. The file to predict landmarks can be found [here](https://github.com/italojs/facial-landmarks-recognition/blob/master/shape_predictor_68_face_landmarks.dat).\n",
    "\n",
    "### Feature Extraction\n",
    "Since facial expression of a person largely depends on the relative locations of the facial landmarks with respect each other, they are considered good candidates to be included in the feature vector. To better determine the relative position of landmark points with respect to each other, they are represented with respect to a new coordinate referrence which is the mean of all the 68 facial landmark points. \n",
    "In some images, the face might not have a straight pose with respect to the vertical axis and might be in a tilted position. To solve this issue, all images in the dataset are rotated so that the faces are aligned with the vertical axis. To determine the rotation angle, 4 points from the 68 facial landmarks are considered. These points are located on the nose and can help us determine the desired rotation angle. In the picture below, the 4 points (27, 28, 29, 30) are shown.\n",
    "\n",
    "\n",
    "![](data/jupyter-data/image1.png)\n",
    "\n",
    "\n",
    "After rotating each facial landmark with the calculated rotation angle, new coordinates are calculated for each landmark with respect to the new referrence point (mean point). Next, the polar coordinates of the points are calculated. First, the distance between each point and the referrence point is calulated. Then, the angle of each point with respect to the horizontal axis is calculated. The idea to use these features is inspired by [this paper](https://arxiv.org/pdf/1603.09129.pdf).\n",
    "\n",
    "### Feature Vector\n",
    "The feature vector for each landmark point consists of 4x68=272 numbers: 68 x coordinates (cartesian coordinates wrt mean point), 68 y coordinates ((cartesian coordinates wrt mean point)), 68 distance values (r in polar coordinate wrt mean point), and 68 angle values (theta in polar coordinates wrt mean point).\n",
    "\n",
    "### Classification\n",
    "The data is split into two parts: training and prediction. 90% of the dataset images are for training and the rest is for prediction/test phase. The dataset images for each set are shuffled before selecting them for training or prediction sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Files\n",
    "##### SVM_Data.py\n",
    "This file contains details for preparing the dataset for training and prediction sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import Landmark_Detector\n",
    "import Shuffle\n",
    "\n",
    "def extract_feature(item):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    # read image\n",
    "    image = cv2.imread(item)\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # increase the contrast\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    # extract features\n",
    "    features_vector = Landmark_Detector.Landmark_Detector(clahe_image)\n",
    "    return features_vector\n",
    "\n",
    "def SVM_Data(dataset):\n",
    "    emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\",\n",
    "                \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    prediction_data = []\n",
    "    prediction_labels = []\n",
    "    for emotion in emotions:\n",
    "        print(\"emotion: \", emotion)\n",
    "        # shuffle the set\n",
    "        training, prediction = Shuffle.Shuffle(emotion, dataset)\n",
    "        print(\"  training ...\")\n",
    "        for item in training:\n",
    "            features_vector = extract_feature(item)\n",
    "            if features_vector == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                # append image array to training data list\n",
    "                training_data.append(features_vector[0])\n",
    "                training_labels.append(emotions.index(emotion))\n",
    "        print(\"  prediction ...\")\n",
    "        for item in prediction:\n",
    "            features_vector = extract_feature(item)\n",
    "            if features_vector == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                # append image array to prediction data list\n",
    "                prediction_data.append(features_vector[0])\n",
    "                prediction_labels.append(emotions.index(emotion))\n",
    "        print()\n",
    "    return training_data, training_labels, prediction_data, prediction_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Landmark_Detector.py\n",
    "This files contains the details for detecting facial landmarks and preparing them to be included in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "def Landmark_Detector(image):\n",
    "    features_vector = []\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\n",
    "        \"data/shape_predictor_68_face_landmarks.dat\")\n",
    "    detections = detector(image, 1)\n",
    "    # repeat for all detected faces\n",
    "    for k, d in enumerate(detections):\n",
    "        # facial landmarks with the predictor class of the dlib library\n",
    "        shape = predictor(image, d)\n",
    "        xsequence = []\n",
    "        ysequence = []\n",
    "        # save x and y coordinates in two lists\n",
    "        for i in range(1, 68):\n",
    "            xsequence.append(float(shape.part(i).x))\n",
    "            ysequence.append(float(shape.part(i).y))\n",
    "        # mean of all landmarks' x and y coordinates\n",
    "        xmean = np.mean(xsequence)\n",
    "        ymean = np.mean(ysequence)\n",
    "        # distance between each point and the mean point\n",
    "        xrelative = [(x-xmean) for x in xsequence]\n",
    "        yrelative = [(y-ymean) for y in ysequence]\n",
    "        # prevent 'divide by 0' error\n",
    "        if xsequence[26] == xsequence[29]:\n",
    "            anglenose = 0\n",
    "        else:\n",
    "            # rotation angle\n",
    "            anglenose = int(math.atan(\n",
    "                (ysequence[26]-ysequence[29])/(xsequence[26]-xsequence[29]))*180/math.pi)\n",
    "        if anglenose < 0:\n",
    "            anglenose += 90\n",
    "        else:\n",
    "            anglenose -= 90\n",
    "        # create feature vector\n",
    "        feature_vector = []\n",
    "        for x, y, w, z in zip(xrelative, yrelative, xsequence, ysequence):\n",
    "            feature_vector.append(x)\n",
    "            feature_vector.append(y)\n",
    "            meannp = np.asarray((ymean, xmean))\n",
    "            coornp = np.asarray((z, w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            if (w - xmean == 0):\n",
    "                if (z - ymean > 0):\n",
    "                    anglerelative = 90 - anglenose\n",
    "                else:\n",
    "                    anglerelative = -90 - anglenose\n",
    "            else:\n",
    "                anglerelative = (math.atan((z-ymean)/(w-xmean))\n",
    "                                 * 180/math.pi) - anglenose\n",
    "            feature_vector.append(dist)\n",
    "            feature_vector.append(anglerelative)\n",
    "        # append to features_vector\n",
    "        features_vector.append(feature_vector)\n",
    "    # return error if no face is detected\n",
    "    if len(detections) < 1:\n",
    "        features_vector = \"error\"\n",
    "    return features_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Below you can see the confusion matrix and the resulting accuracy:\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "![](data/jupyter-data/image2.png)\n",
    "\n",
    "Accuracy: 0.93103448\n",
    "\n",
    "### Test on new images\n",
    "\n",
    "Image 1:\n",
    "\n",
    "![](data/jupyter-data/test_pic2.jpg)\n",
    "\n",
    "### Links to other materials\n",
    "1. Link to YouTube Video: https://youtu.be/7ht-pKO7nGI\n",
    "\n",
    "2. Link to Supplemantary Materials: https://github.com/am-nu-fall-22/emotion-recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
